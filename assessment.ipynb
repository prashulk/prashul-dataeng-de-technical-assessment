{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineer - Technical Assessment\n",
    "\n",
    "In this section of the interview at Beyond Finance, you will be assessed on your ability to perform several Data Engineering tasks. To perform well on this task, you will demonstate competence in the following areas:\n",
    "\n",
    "* preprocessing data to prepare for a database load\n",
    "* understanding entity relationships in a database\n",
    "* merging data from different tables\n",
    "* filtering data to relevant subsets\n",
    "* calculating aggregations and descriptive statistics\n",
    "\n",
    "It will be pretty difficult to complete all questions in the allotted time. Your goal is not to speed through the answers, but to come up with answers that demonstrate your knowledge. It's more about your thought process and logic than getting the right answer or your code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "This exercise will be broken into 2 parts\n",
    "1. Data Processing\n",
    "2. Data Wrangling\n",
    "\n",
    "### Data Processing\n",
    "In this section you will take files from the ./raw_data/ subfolders, combine them into a single newline-delimited `json.gz` file per subfolder, and place that CSV file in a ./processed_data/ directory. You may have to do some light investigation into the data files to understand their file formats and delimiters\n",
    "\n",
    "**Example**\n",
    "\n",
    "Files\n",
    "- ./raw_data/tracks/tracks_0.csv\n",
    "- ./raw_data/tracks/tracks_1.json\n",
    "- ./raw_data/tracks/tracks_2.csv\n",
    "- etc... \n",
    "\n",
    "should be combined into a single file ./processed_data/tracks.json.gz\n",
    "\n",
    "**What we look for**\n",
    "\n",
    "- Can you handle all subfolders in a single pass over the raw data files?\n",
    "    \n",
    "    Yes, I designed the script so that each sub-folder is touched exactly once and every raw file inside it is streamed only once.\n",
    "\n",
    "    1. run_all() loops over the dataset folders single time and inside process() the files are sorted and read each one sequentially. The CSV reader uses chunksize and JSON reader goes line by line.\n",
    "\n",
    "    2. When reading, i directly write to gzip(or append to it if resuming after checkpoint), so there is no second pass\n",
    "\n",
    "- What if the file sizes are in GigaBytes? Can your code (if run on a standard laptop) load the files without going out of memory? (hint `chunksize`)\n",
    "\n",
    "    Yes. I read CSVs in chunks and JSON files line-by-line, writing each record straight to the gzip. That means only one chunk or one line sits in memory at once, so multi-GB source files run fine on a standard laptop without exhausting RAM.\n",
    "\n",
    "- Can you identify edge cases? What scenarios could break your code?\n",
    "\n",
    "    For simplicity as of now data is written as is, but if not stronger checks are applied it would break due to following reasons\n",
    "    \n",
    "    1. Duplicate data:\n",
    "     As of now I observed the entire files data are duplicated. Assuming we have GBs/TBs of files I only flag the first exact duplicate via a whole-row SHA-1 and keep loading; removing them in Python would cost more RAM/CPU than it saves. If the files were small I’d dedupe in-memory, but for GB-scale loads I assume true dedup happens later in the warehouse (e.g. silver layer processing).\n",
    "\n",
    "    2. Schema drift:\n",
    "        Whenever a column name first appears I log it and pass the data through unchanged, so nothing is lost (and the table will grow horizontally when new columns appear). This can be handled in downstream layers using mergeschema/ rename.\n",
    "\n",
    "    3. Invalid data-types:\n",
    "     A price that arrives as a string or a date in a different format is written as is. The assumption is that stronger type-checks happen in curated layers.\n",
    "\n",
    "    4. Incorrect JSON – Any line that fails json.loads is skipped and logged, keeping the data load alive as of now but that needs to be handled correctly.\n",
    "\n",
    "    5. Mid-run failures:\n",
    "     Data lands in *.json.gz.part and a checkpoint records the last raw file completed. If the job stops due to any reason the partial file and checkpoint remain, so we can rerun or simply delete and reload without assuming partial output for finished data.\n",
    "\n",
    "\n",
    "- Please directly respond to the above questions in your submission.\n",
    "\n",
    "### Data Wrangling\n",
    "For this section, we'll pretend you loaded the raw data plus additional tables into a small SQLite database containing roughly a dozen tables. **We've provided this database for you so don't worry about loading it yourself**. If you are not familiar with the SQLite database, it uses a fairly complete and standard SQL syntax, though does not many advanced analytics functions. Consider it just a remote datastore for storing and retrieving data from. \n",
    "\n",
    "![](db-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "#!pip install memory_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, gzip, logging, os, pandas as pd\n",
    "from hashlib import sha1\n",
    "from datetime import datetime\n",
    "\n",
    "RAW_DIR = Path(\"raw_data\")\n",
    "OUT_DIR = Path(\"processed_data\")\n",
    "LOG_DIR = Path(\"logs\")\n",
    "\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "log_path = LOG_DIR / f\"run_{datetime.now():%Y%m%d_%H%M%S}.log\"\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.INFO)\n",
    "root.handlers.clear()\n",
    "fh = logging.FileHandler(log_path, mode=\"w\", encoding=\"utf-8\")\n",
    "fh.setFormatter(logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\"))\n",
    "root.addHandler(fh)\n",
    "root.info(\"job started\")\n",
    "\n",
    "#utility functions\n",
    "def clean_cols(rec):\n",
    "    out = {}\n",
    "    for k, v in rec.items():\n",
    "        out[k.split(\".\")[-1]] = v\n",
    "    return out\n",
    "\n",
    "def row_hash(rec):\n",
    "    s = json.dumps(rec, sort_keys=True)\n",
    "    return sha1(s.encode()).digest()\n",
    "\n",
    "def csv_rows(path, cols):\n",
    "    for chunk in pd.read_csv(path, chunksize=50_000):\n",
    "        for r in chunk.to_dict(orient=\"records\"):\n",
    "            r = clean_cols(r)\n",
    "            for c in r:\n",
    "                cols.add(c)\n",
    "            yield r\n",
    "\n",
    "def json_rows(path, cols, stats):\n",
    "    with path.open() as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                r = clean_cols(json.loads(line))\n",
    "                for c in r:\n",
    "                    cols.add(c)\n",
    "                yield r\n",
    "            except json.JSONDecodeError:\n",
    "                stats[\"bad_json\"] += 1\n",
    "                logging.warning(\"bad json in %s\", path)\n",
    "\n",
    "def load_ckpt(cp):\n",
    "    if cp.exists():\n",
    "        with cp.open() as f:\n",
    "            name = f.readline().strip()\n",
    "            rows = int(f.readline().strip() or 0)\n",
    "            return name, rows\n",
    "    return None, 0\n",
    "\n",
    "def save_ckpt(cp, filename, rows):\n",
    "    with cp.open(\"w\") as f:\n",
    "        f.write(f\"{filename}\\n{rows}\\n\")\n",
    "\n",
    "#Process data\n",
    "def process(ds):\n",
    "    folder = RAW_DIR / ds\n",
    "    if not folder.is_dir():\n",
    "        print(\"missing:\", ds)\n",
    "        return\n",
    "\n",
    "    ckpt = OUT_DIR / f\"{ds}.ckpt\"\n",
    "    last_done, rows_done = load_ckpt(ckpt)\n",
    "\n",
    "    part = OUT_DIR / f\"{ds}.json.gz.part\"\n",
    "    final = OUT_DIR / f\"{ds}.json.gz\"\n",
    "    mode  = \"at\" if rows_done else \"wt\"\n",
    "\n",
    "    stats = {\"rows\": rows_done, \"bad_json\": 0, \"dup_seen\": False}\n",
    "    cols_seen, hashes = set(), set()\n",
    "\n",
    "    try:\n",
    "        gz = gzip.open(part, mode, encoding=\"utf-8\")\n",
    "\n",
    "        for src in sorted(folder.iterdir()):\n",
    "            if last_done and src.name <= last_done:\n",
    "                continue\n",
    "\n",
    "            ext = src.suffix.lower()\n",
    "            if ext == \".csv\":\n",
    "                reader = csv_rows(src, cols_seen)\n",
    "            elif ext in (\".json\"):\n",
    "                reader = json_rows(src, cols_seen, stats)\n",
    "            else:\n",
    "                logging.info(\"skipped %s\", src)\n",
    "                continue\n",
    "\n",
    "            for rec in reader:\n",
    "                h = row_hash(rec)\n",
    "                if not stats[\"dup_seen\"] and h in hashes:\n",
    "                    stats[\"dup_seen\"] = True\n",
    "                    logging.info(\"duplicate rows detected in %s\", ds)\n",
    "                hashes.add(h)\n",
    "\n",
    "                new_cols = []\n",
    "                for col in rec:\n",
    "                    if col not in cols_seen:\n",
    "                        new_cols.append(col)\n",
    "                        cols_seen.add(col)\n",
    "                if new_cols:\n",
    "                    logging.info(\"new columns in %s: %s\", ds, \", \".join(new_cols))\n",
    "\n",
    "                gz.write(json.dumps(rec) + \"\\n\")\n",
    "                stats[\"rows\"] += 1\n",
    "\n",
    "            save_ckpt(ckpt, src.name, stats[\"rows\"])\n",
    "\n",
    "        gz.close()\n",
    "        os.replace(part, final)\n",
    "        ckpt.unlink(missing_ok=True)\n",
    "        print(f\"{ds}: rows={stats['rows']}  bad_json={stats['bad_json']}  \"\n",
    "              f\"dup_seen={'yes' if stats['dup_seen'] else 'no'}  columns={len(cols_seen)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        gz.close()\n",
    "        logging.exception(\"pipeline stopped in %s: %s\", ds, e)\n",
    "        print(\"error processing\", ds, \"-\", e)\n",
    "        return\n",
    "\n",
    "def run_all():\n",
    "    for d in (\"orders\", \"playlist_track\", \"track_facts\", \"tracks\"):\n",
    "        process(d)\n",
    "    logging.shutdown()\n",
    "    print(\"log file:\", log_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orders: rows=1120000  bad_json=0  dup_seen=yes  columns=6\n",
      "playlist_track: rows=4357500  bad_json=0  dup_seen=yes  columns=2\n",
      "track_facts: rows=1751500  bad_json=0  dup_seen=yes  columns=3\n",
      "tracks: rows=1751500  bad_json=0  dup_seen=yes  columns=9\n",
      "log file: logs/run_20250426_160855.log\n",
      "peak memory: 190.25 MiB, increment: 96.33 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipython-sql\n",
      "  Downloading ipython_sql-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting prettytable (from ipython-sql)\n",
      "  Downloading prettytable-3.16.0-py3-none-any.whl.metadata (33 kB)\n",
      "Requirement already satisfied: ipython in /Users/prashulkumar/anaconda3/envs/myenv/lib/python3.10/site-packages (from ipython-sql) (8.25.0)\n",
      "Collecting sqlalchemy>=2.0 (from ipython-sql)\n",
      "  Downloading sqlalchemy-2.0.40-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting sqlparse (from ipython-sql)\n",
      "  Downloading sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: six in /Users/prashulkumar/anaconda3/envs/myenv/lib/python3.10/site-packages (from ipython-sql) (1.16.0)\n",
      "Collecting ipython-genutils (from ipython-sql)\n",
      "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl.metadata (755 bytes)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Users/prashulkumar/anaconda3/envs/myenv/lib/python3.10/site-packages (from sqlalchemy>=2.0->ipython-sql) (4.12.2)\n",
      "Requirement already satisfied: decorator in /Users/prashulkumar/anaconda3/envs/myenv/lib/python3.10/site-packages (from ipython->ipython-sql) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/prashulkumar/anaconda3/envs/myenv/lib/python3.10/site-packages (from ipython->ipython-sql) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/prashulkumar/anaconda3/envs/myenv/lib/python3.10/site-packages (from ipython->ipython-sql) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/prashulkumar/anaconda3/envs/myenv/lib/python3.10/site-packages (from ipython->ipython-sql) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/prashulkumar/anaconda3/envs/myenv/lib/python3.10/site-packages (from ipython->ipython-sql) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /Users/prashulkumar/anaconda3/envs/myenv/lib/python3.10/site-packages (from ipython->ipython-sql) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /Users/prashulkumar/anaconda3/envs/myenv/lib/python3.10/site-packages (from ipython->ipython-sql) (5.14.3)\n",
      "Requirement already satisfied: exceptiongroup in /Users/prashulkumar/anaconda3/envs/myenv/lib/python3.10/site-packages (from ipython->ipython-sql) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/prashulkumar/anaconda3/envs/myenv/lib/python3.10/site-packages (from ipython->ipython-sql) (4.9.0)\n",
      "Requirement already satisfied: wcwidth in /Users/prashulkumar/anaconda3/envs/myenv/lib/python3.10/site-packages (from prettytable->ipython-sql) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/prashulkumar/anaconda3/envs/myenv/lib/python3.10/site-packages (from jedi>=0.16->ipython->ipython-sql) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/prashulkumar/anaconda3/envs/myenv/lib/python3.10/site-packages (from pexpect>4.3->ipython->ipython-sql) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/prashulkumar/anaconda3/envs/myenv/lib/python3.10/site-packages (from stack-data->ipython->ipython-sql) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/prashulkumar/anaconda3/envs/myenv/lib/python3.10/site-packages (from stack-data->ipython->ipython-sql) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/prashulkumar/anaconda3/envs/myenv/lib/python3.10/site-packages (from stack-data->ipython->ipython-sql) (0.2.2)\n",
      "Downloading ipython_sql-0.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading sqlalchemy-2.0.40-cp310-cp310-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Downloading prettytable-3.16.0-py3-none-any.whl (33 kB)\n",
      "Downloading sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ipython-genutils, sqlparse, sqlalchemy, prettytable, ipython-sql\n",
      "Successfully installed ipython-genutils-0.2.0 ipython-sql-0.5.0 prettytable-3.16.0 sqlalchemy-2.0.40 sqlparse-0.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install ipython-sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql \n",
    "%sql sqlite:///db/sqlite/chinook.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "con = sqlite3.connect(\"db/sqlite/chinook.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of tables: [('albums',), ('sqlite_sequence',), ('artists',), ('customers',), ('employees',), ('genres',), ('invoices',), ('invoice_items',), ('media_types',), ('playlists',), ('playlist_track',), ('tracks',), ('sqlite_stat1',)]\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "print(\"List of tables:\", [table for table in tables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CustomerId  FirstName     LastName  \\\n",
      "0           1       Luís    Gonçalves   \n",
      "1           2     Leonie       Köhler   \n",
      "2           3   François     Tremblay   \n",
      "3           4      Bjørn       Hansen   \n",
      "4           5  František  Wichterlová   \n",
      "\n",
      "                                            Company  \\\n",
      "0  Embraer - Empresa Brasileira de Aeronáutica S.A.   \n",
      "1                                              None   \n",
      "2                                              None   \n",
      "3                                              None   \n",
      "4                                  JetBrains s.r.o.   \n",
      "\n",
      "                           Address                 City State         Country  \\\n",
      "0  Av. Brigadeiro Faria Lima, 2170  São José dos Campos    SP          Brazil   \n",
      "1          Theodor-Heuss-Straße 34            Stuttgart  None         Germany   \n",
      "2                1498 rue Bélanger             Montréal    QC          Canada   \n",
      "3                 Ullevålsveien 14                 Oslo  None          Norway   \n",
      "4                    Klanova 9/506               Prague  None  Czech Republic   \n",
      "\n",
      "  PostalCode               Phone                 Fax  \\\n",
      "0  12227-000  +55 (12) 3923-5555  +55 (12) 3923-5566   \n",
      "1      70174    +49 0711 2842222                None   \n",
      "2    H2G 1A7   +1 (514) 721-4711                None   \n",
      "3       0171     +47 22 44 22 22                None   \n",
      "4      14700    +420 2 4172 5555    +420 2 4172 5555   \n",
      "\n",
      "                      Email  SupportRepId  \n",
      "0      luisg@embraer.com.br             3  \n",
      "1     leonekohler@surfeu.de             5  \n",
      "2       ftremblay@gmail.com             3  \n",
      "3     bjorn.hansen@yahoo.no             4  \n",
      "4  frantisekw@jetbrains.com             4  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM customers LIMIT 5;\", con)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How many different customers are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct customers: 59\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT COUNT(DISTINCT CustomerId) FROM customers\")\n",
    "count_row = cursor.fetchone()[0]\n",
    "print(\"Distinct customers:\", count_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How long is the longest track in minutes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Name  Milliseconds  length_minutes\n",
      "0       Occupation / Precipice       5286953           88.12\n",
      "1      Through a Looking Glass       5088838           84.81\n",
      "2  Greetings from Earth, Pt. 1       2960293           49.34\n",
      "3      The Man With Nine Lives       2956998           49.28\n",
      "4  Battlestar Galactica, Pt. 2       2956081           49.27\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT Name, Milliseconds, ROUND(Milliseconds /(1000*60.0), 2) AS length_minutes\n",
    "    FROM tracks\n",
    "    ORDER BY 2 DESC\n",
    "    LIMIT 5;\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, con)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest track length (minutes): ('Occupation / Precipice', 88.12)\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\n",
    "    \"\"\"\n",
    "    SELECT Name, ROUND(Milliseconds /(1000*60.0), 2) AS length_minutes\n",
    "    FROM tracks\n",
    "    ORDER BY 2 DESC\n",
    "    LIMIT 1;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "max_minutes = cursor.fetchone()\n",
    "print(\"Longest track length (minutes):\", max_minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Which genre has the shortest average track length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Name  avg_length_minutes\n",
      "0   Rock And Roll                2.24\n",
      "1           Opera                2.91\n",
      "2     Hip Hop/Rap                2.97\n",
      "3  Easy Listening                3.15\n",
      "4      Bossa Nova                3.66\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    select g.Name, ROUND(AVG(t.Milliseconds) / (1000*60.0), 2) AS avg_length_minutes\n",
    "    from tracks t, genres g\n",
    "    where t.GenreId = g.GenreId\n",
    "    group by 1\n",
    "    order by 2\n",
    "    limit 5;\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, con)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track with shortest avg length ('Rock And Roll', 2.24)\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\n",
    "    \"\"\"\n",
    "    select g.Name, ROUND(AVG(t.Milliseconds) / (1000*60.0), 2) AS avg_length_minutes\n",
    "    from tracks t, genres g\n",
    "    where t.GenreId = g.GenreId\n",
    "    group by 1\n",
    "    order by 2\n",
    "    limit 1;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "shortest_avg = cursor.fetchone()\n",
    "print(\"track with shortest avg length\", shortest_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Which artist shows up in the most playlists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PlaylistId\n",
      "0           1\n",
      "1           3\n",
      "2           5\n",
      "3           8\n",
      "4           9\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select distinct PlaylistId from playlist_track\n",
    "    limit 5;\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, con)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Name  \\\n",
      "0                                     Eugene Ormandy   \n",
      "1                                 The King's Singers   \n",
      "2                   English Concert & Trevor Pinnock   \n",
      "3      Berliner Philharmoniker & Herbert Von Karajan   \n",
      "4  Academy of St. Martin in the Fields & Sir Nevi...   \n",
      "\n",
      "   count(distinct pt.PlaylistId)  \n",
      "0                              7  \n",
      "1                              6  \n",
      "2                              6  \n",
      "3                              6  \n",
      "4                              6  \n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    select a.Name, count(distinct pt.PlaylistId)\n",
    "    from artists a, albums al, tracks t, playlist_track pt\n",
    "    where a.ArtistId = al.ArtistId\n",
    "    and al.AlbumId = t.AlbumId\n",
    "    and t.TrackId = pt.TrackId\n",
    "    group by 1\n",
    "    order by 2 desc\n",
    "    limit 5;\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, con)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artist in most playlist ('Eugene Ormandy', 7)\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\n",
    "    \"\"\"\n",
    "    select a.Name, count(distinct pt.PlaylistId)\n",
    "    from artists a, albums al, tracks t, playlist_track pt\n",
    "    where a.ArtistId = al.ArtistId\n",
    "    and al.AlbumId = t.AlbumId\n",
    "    and t.TrackId = pt.TrackId\n",
    "    group by 1\n",
    "    order by 2 desc\n",
    "    limit 1;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "artist = cursor.fetchone()\n",
    "print(\"artist in most playlist\", artist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. What album had the most purchases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Title  InvoiceLineId  InvoiceId  UnitPrice  Quantity\n",
      "0  ...And Justice For All            317         60       0.99         1\n",
      "1  ...And Justice For All            890        165       0.99         1\n",
      "2  ...And Justice For All            891        165       0.99         1\n",
      "3  ...And Justice For All           1464        270       0.99         1\n",
      "4  ...And Justice For All           1465        270       0.99         1\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    select al.Title, i.InvoiceLineid, i.InvoiceId, i.UnitPrice, i.Quantity\n",
    "    from albums al, tracks t, invoice_items i\n",
    "    where al.AlbumId = t.AlbumId\n",
    "    and t.TrackId = i.TrackId\n",
    "    order by 1,2 \n",
    "    limit 5;\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, con)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "album with most purchase ('Minha Historia', 27)\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\n",
    "    \"\"\"\n",
    "    select al.Title, count(distinct i.InvoiceLineid)\n",
    "    from albums al, tracks t, invoice_items i\n",
    "    where al.AlbumId = t.AlbumId\n",
    "    and t.TrackId = i.TrackId\n",
    "    group by 1\n",
    "    order by 2 desc\n",
    "    limit 1;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "album = cursor.fetchone()\n",
    "print(\"album with most purchase\", album)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Which customer has the highest number of sales in terms of dollars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer with highest sales:  Helena Holý\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\n",
    "    \"\"\"\n",
    "    select concat(c.FirstName, ' ', c.LastName) as customer_name\n",
    "    from customers c, invoices i\n",
    "    where c.CustomerId = i.CustomerId\n",
    "    group by 1\n",
    "    order by sum(i.Total) desc\n",
    "    limit 1;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "cust = cursor.fetchone()[0]\n",
    "print(\"customer with highest sales: \", cust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Count of customers who have dollar sales more than $40?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer with sales > 40:  14\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\n",
    "    \"\"\"\n",
    "    with base as (select customerId, sum(Total) from invoices\n",
    "    group by 1\n",
    "    having sum(Total) > 40)\n",
    "    select count(customerId) from base\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "greaterthan40 = cursor.fetchone()[0]\n",
    "print(\"customer with sales > 40: \", greaterthan40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
